import time 
import numpy as np
from scipy import stats
from sklearn.cluster import SpectralClustering
from sklearn.cluster import KMeans
from scipy.stats import norm
import statsmodels.stats.multitest
from sklearn.impute import SimpleImputer


from Pipeline.CBASS_L3_MergeFilters import MergeFilters
from Pipeline.CBASS_L3_MakeFilter import MakeFilter
from Pipeline.Utilities.CBASS_U_SpectralClustering import L,SC
from Pipeline.CBASS_Plot_Filters import PlotFilters
from Pipeline.Utilities.CBASS_U_GaussianKernel import gaussian_kernel

# Temporarily removing until I figure out what is happening with networkx
from Pipeline.CBASS_Plot_Graphs import PlotGraphs 
import networkx as nx
import community as community_louvain


def GetFilters(sREC, sTROUGH, sTRGH_RND, bl1Epoch, sOPTION): #blZScore, inNClu, verbose=False):
    '''
     L2 of the bout pipeline: Generate template motifs of activity in the band
     of interst enriched for the state indexed by the logical vector bl1Epoch.
     The algorithm separates a set of events of the input signal sREC.db2LFP
     into an aribitrary number of clusters using the k-means algorithm. These
     events are the output of CBASS_L1_GetTrough and correspond to the trougths
     of oscilatory activity at the band of interest in a reference channel.
     They are represented as the Hilbert transform of the band filtered
     activity of each channel. Clusters are then sorted as a function of the
     fraction of the events they comprise that occured during the state
     defined by bl1Epoch. Clusters that are significantly 'enriched' in events
     occuring during this state are used to defined templates activity motifs.
     These motifs represent the average signal activity around the events of
     the clusters. Enriched clusters are then grouped as a function of the
     similarity of the motifs they generate using spectral clustering. Grouped
     clusters are used to generate a final set of template activity motifs
     returned in the function sFILTER.

     Input -------------------------------------------------------------------

     sREC:         a structure having the following fields:
                   -.db2LFP a (channel x time sample) matrix containing the
                   signal (i.e. time series) of interest. 
                   -.db2LFP_Rnd a (channel x time sample) matrix containing 
                   the surrogate signal (generated by 
                   CBASS_L1_AddPhaseRandomizedSignal)
                   -.inSampleRate a positive number representing the sample
                   rate of the time series.
     sTROUGH:      the output of CBASS_L1_GetTrough (i.e.) a structure
                   requiring the following fields:
                   -.db1FilterBand an (1 x 2) array describing the frequency
                   band of interest i.e. [30 80] for 30 to 80 Hz.
                   -.db2Trough  a (2 * channel x trough) matrix containing the
                   hilbert transform of each channel of sREC.db2LFP filtered
                   in the band defined in sTROUGH.db1FilterBand at the trough
                   of the filtered signal in a reference channel (see
                   CBASS_L1_GetTrough for more detail)
                   -.in1Index the indices of the trough in sREC.db2LFP
     sTRGH_RND:    a structure formatted as sTROUGH containing troughs
                   computed from the surrogate data (generated by
                   CBASS_L1_AddPhaseRandomizedSignal and stored in
                   sREC.db2LFP_Rnd).
     bl1Epopch:    a logical vector, containing as many elements as time
                   samples in db2LFP, indexing the state in which enriched
                   band specific activity is observed.
     bl1Baseline:  (optional) a logical vector, containing as many elements as
                   time samples in db2LFP, indexing the state in which band
                   specific activity is not observed. There should be no
                   overlap between bl1Baseline and bl1Epoch. Default is
                   ~bl1Epoch.
     blZScore:     (optional) logical specifying if trought data is to be
                   zscored for k-means partitioning. Default is true.
     inMethod:     (optional) single integer with value 1 or 2. Determines the
                   method used to set the threshold of similarity used to
                   retain an edge in spectral clustering during the grouping
                   of templates. Method 1 sets a general threshold as the
                   average similarity of templates identified on surrogate
                   data. Method 2 sets a threshold for each edge as the
                   maximum simarity between its nodes and a chance template
                   computed as the average activity around all troughs in the
                   surrogate data. Method 2 identifies more templates, but
                   these templates will generate overlapping sets of pulses.
                   Default is 1.
     inNClu:       (optional) number of cluster used for k-means partitioning.
                   Default is 20
     dbSigThrs:    (optional) threshold for the significance of the enrichment
                   in trough partition.  P-Values are computed with a binomial 
                   test. Default is 10.^-4.
     inNMaxIter:   (optional) maximum iteration used by the k-means algorithm.
                   Default is 10000.
     blVerbose:    (optional) logical setting whether processing updates
                   should be displayed in the command window. Default is true.

     Output ------------------------------------------------------------------

     sFILTER       a structure array storing templates for activity motifs and
                   containing the following fields:
                   -.in1CluSel a vector containing the indices of the k-means
                   clusters used to build the filter
                   -.bl1Member a boolean indexing troughs used to build the
                   template motif
                   -.inNObs the number of troughs in sFILTER.bl1Member.
                   -.dbRate the fraction of the troughs occuring during the
                   state of interest.
                   -.dbRate_Dev the enrichment for the state of interest i.e.
                   sFILTER.dbRate minus the overall ratio of troughs occuring
                   during the state of interest
                   -.dbPVal the p-value of a binomial test of the of dbRate
                   compared to the overall rate of occurence of the state of
                   interest.
                   -.db2Filter a (channel x time sample) matrix describing the
                   template motif. The number of time samples is one period of
                   the median frequency of the band of interest.
     sCLU          a structure array storing the clusters of the k-means
                   partitioning and having the following fields:
                   -.bl1Member a boolean indexing troughs used to build the
                   template motif
                   -.inNObs the number of troughs in sCLU.bl1Member.
                   -.dbRate the fraction of the troughs occuring during the
                   state of interest.
                   -.dbRate_Dev the enrichment for the state of interest i.e.
                   sCLU.dbRate minus the overall ratio of troughs occuring
                   during the state of interest
                   -.dbPVal the p-value of a binomial test of the of dbRate
                   compared to the overall rate of occurence of the state of
                   interest.

    '''
    verbose = sOPTION.blVerbose
    start = time.time()
    # Zscore
    db2Data =stats.zscore(sTROUGH.db2Trough, axis=0, ddof=1)
    if verbose:
        print('np.isnan(db2Data).any(): ', np.isnan(db2Data).any())
        print('np.isinf(db2Data).any(): ', np.isinf(db2Data).any())
        print('np.isinf(db2Data).any(): ', np.isinf(db2Data).any())
        print('np.min(db2Data): ', np.min(db2Data))
        print('np.max(db2Data): ', np.max(db2Data))
    if np.isnan(db2Data).any():
        # Create our imputer to replace missing values with the mean
        imp = SimpleImputer(missing_values=np.nan, strategy='mean')
        imp = imp.fit(db2Data)
        
        # Impute our data
        db2Data = imp.transform(db2Data)

    ## Cluster the data 
    print('sOPTION.ClusterMETHOD: ',sOPTION.ClusterMETHOD)
    if sOPTION.ClusterMETHOD=='SpecClusters':
        
        if sOPTION.blSklearn: # Built-in
            in1CluKM = SpectralClustering(n_clusters=sOPTION.inNClu, assign_labels="discretize",random_state=0,n_jobs=-1).fit_predict(db2Data)
        else: # My implementation
            W = gaussian_kernel(db2Data, kernel_type="adaptive", sigma=None) # Build an adjacency matrix for data. Each column here is a filter (aka, observation)
            Lap = L(W,normalized=False); # Compute a graph laplacian
            in1CluKM, _ = SC(Lap, k=sOPTION.inNClu, psi=None, nrep=5, itermax=300, show_plot=True, topK = 5, verbose=False) # Compute labels and number of clusters according via spectral clustering


    elif sOPTION.ClusterMETHOD=='Kmeans':
        if sOPTION.blSklearn:
            in1CluKM = KMeans(n_clusters=sOPTION.inNClu, n_init=5, verbose=False).fit_predict(db2Data) # with kmeans. 
        else: 
            print('Still missing...')
    end = time.time()
    print('Time for computing the clustering: {}'.format(end-start))
    
    '''
    Checks if clusters are enriched for the different states (eg, visual stimuli or running)
    '''
    start = time.time()
    # Computes the indices of the bouts where the mouse was presented a visual stimuli or running 
    bl1Epoch = bl1Epoch.squeeze()
    bl1T_Epoch  = bl1Epoch[sTROUGH.in1Index];
    if verbose: print('bl1T_Epoch.shape: ',bl1T_Epoch.shape)

    # Computes the overall ratio of running/stimulus points in the data
    dbRate_All = bl1T_Epoch.mean();
    if verbose: print('dbRate_All: ',dbRate_All)
        
    '''
    Initialize a cluster structure containing stats for enrichment
    '''
    class sCLU:
        pass

    sCLU = sCLU()
    sCLU.bl1Member = []
    sCLU.inNObs = []
    sCLU.dbRate = []
    sCLU.dbRate_Dev = []
    sCLU.dbPVal = []

    # Loop over clusters
    for iClu in range(sOPTION.inNClu):
        bl1Member   = np.where(in1CluKM == iClu)[0]; # indices of the cluster
        inNObs      = len(bl1Member) #np.sum(bl1Member); # number of points in the cluster
        sCLU.bl1Member.append(bl1Member)
        sCLU.inNObs.append(inNObs)

        '''
        Calculate the ratio of running points for the cluster and the p-value
        of that ratio under H0 i.e. the hypothesis that the ratio is the same
        that in the overall population. The p-value is calculated using a
        binomial distribution.
        '''
        
        dbRate                  = np.mean(bl1T_Epoch[bl1Member]) # Calculates the ratio
        dbRate_Dev              = dbRate - dbRate_All;
        sCLU.dbRate.append(dbRate)       # Stores the ratio
        sCLU.dbRate_Dev.append(dbRate_Dev)   # Stores the devation
        pval = np.nanmin([1, 2 * (1 - norm.cdf(np.abs(dbRate_Dev)/np.sqrt(dbRate_All * (1 - dbRate_All)/ inNObs)))])
        if verbose: print('np.abs(dbRate_Dev): {:.4f}, \tnp.sqrt(dbRate_All * (1 - dbRate_All)/ inNObs): {:.4f}, \tpval: {:.4f}'.format(np.abs(dbRate_Dev), np.sqrt(dbRate_All * (1 - dbRate_All)/ inNObs), pval))
        sCLU.dbPVal.append(pval)
    
    '''
    Determine significance of the groups via Benjamini/Hochberg (non-negative) multiple test
    '''

#     dbSigThrs = 0.0001
    cSIG = statsmodels.stats.multitest.multipletests(sCLU.dbPVal, alpha=sOPTION.dbSigThrs, method='fdr_bh')
    if verbose: print('cSIG: \n',cSIG)
    sCLU.blSig_FDR = cSIG[0]
    
    ## Print the results
    # Sort the cluster by ratio
    in1Sort = np.array(sCLU.dbRate_Dev).argsort();
    if verbose: print('np.array(sCLU.dbRate_Dev)[in1Sort]: \n',np.array(sCLU.dbRate_Dev)[in1Sort])

    # Initialize a significance statement array
    cSTATEMENT = ['NON SIG', 'SIG'];

    # Loops throught clusters
    for iClu in in1Sort:
        print('Cluster {} \t(N = {}):\tRate: {:.4f}\tDev: {:.3f}\t(p = {:.4f})\t{}\r'.format(iClu, np.array(sCLU.inNObs)[iClu], 
                                                                                           np.array(sCLU.dbRate)[iClu], np.array(sCLU.dbRate_Dev)[iClu], 
                                                                                           np.array(sCLU.dbPVal)[iClu], cSTATEMENT[sCLU.blSig_FDR[iClu]]))
        
    in1Sel = np.where((np.array(sCLU.dbRate_Dev) > 0) & (np.array(sCLU.dbPVal) < sOPTION.dbSigThrs))[0];
    if verbose: print('in1Sel: ',in1Sel)
    if len(in1Sel)==0:
        print('No enriched regions');
        sFILTER = [];


    # Sets the filter length
    dbCycLen            = 1/np.mean(sTROUGH.db1FilterBand);
    db1Filter_WinSec    = np.array([-dbCycLen, dbCycLen]) / 2;

    # Get the filter for each enriched region
    cFILTER = []; db2FiltMat = [];
    for iClu in in1Sel:
        # Makes the filter
        in1EventIdx = np.array(sTROUGH.in1Index)[np.array(sCLU.bl1Member)[iClu]];
        db2Filter   = MakeFilter(sREC.db2LFP, sREC.inSampleRate, in1EventIdx, db1Filter_WinSec);

        # Aggregates the pulse events for the cluster of interest
        cFILTER.append(db2Filter)
        db2Filter = db2Filter.reshape(db2Filter.size,-1)
        db2FiltMat.append(db2Filter)

    db2FiltMat = np.concatenate(db2FiltMat,axis=1)
    if verbose: 
        print('len(cFILTER): ',len(cFILTER))
        print('db2FiltMat.shape: ',db2FiltMat.shape)
        
    #  Plot the filter for the enriched states
    print('Original filters')
    PlotFilters(cFILTER)
    end = time.time()
    print('Time to plot filters: {}'.format(end - start))
        
    '''
    Cluster the filters. 
    '''
    if sOPTION.MergingMETHOD=='random_filters':
        '''
        Strategy 1: Sets the threshold as the average correlation fitler generated on random data
        '''
        print('Starting random_filters...')
        start = time.time()
        # Zscore
        db2Data =stats.zscore(sTRGH_RND.db2Trough, axis=0, ddof=1)
        if np.isnan(db2Data).any():
            # Create our imputer to replace missing values with the mean
            imp = SimpleImputer(missing_values=np.nan, strategy='mean')
            imp = imp.fit(db2Data)
            db2Data = imp.transform(db2Data)

        # Cluster the data
        in1CluKM_Rnd = KMeans(n_clusters=sOPTION.inNClu, n_init=5, verbose=False).fit_predict(db2Data) # with kmeans. 
        if verbose: print('in1CluKM_Rnd.shape: ',in1CluKM_Rnd.shape)
            
         # Get the filter for each enriched region
        db2FiltMat_Rnd = [];
        for iClu in in1Sel:
            # Makes the filter
            in1EventIdx = np.array(sTRGH_RND.in1Index)[np.where(in1CluKM_Rnd==iClu)[0]];
            db2Filter   = MakeFilter(sREC.db2LFP, sREC.inSampleRate, in1EventIdx, db1Filter_WinSec);

            # Aggregates the pulse events for the cluster of interest
            db2Filter = db2Filter.reshape(db2Filter.size,-1)
            db2FiltMat_Rnd.append(db2Filter)

        db2FiltMat_Rnd = np.concatenate(db2FiltMat_Rnd,axis=1)
        if verbose: 
            print('db2FiltMat_Rnd.shape: ',db2FiltMat_Rnd.shape)


        # Calculates the correlations between random fitlers and sets the threshold
        db2Corr_Rnd = np.corrcoef(db2FiltMat_Rnd, rowvar=False) # By default, the coffcoef computes correlation between rows. Set rowvar=False to compute corr between columns
        if verbose: print('db2Corr_Rnd.shape: \n',db2Corr_Rnd.shape)
        db1Corr_Rnd   = []; 
        for iEv in range(len(db2Corr_Rnd)-1): #= 1:size(db2Corr_Rnd, 1) - 1
            db1Corr_Rnd.append(db2Corr_Rnd[iEv+1:, iEv].flatten())
        db1Corr_Rnd = np.concatenate(db1Corr_Rnd)
        if verbose: print('db1Corr_Rnd.shape: ',db1Corr_Rnd.shape)
        dbThrsMean  = np.mean(db1Corr_Rnd);
        if verbose: print('dbThrsMean: ',dbThrsMean)

        # Calculate the correlation between fitlters and sets those inferior to the threshold to zero to built the adjacency matrix
        db2Adj = np.corrcoef(db2FiltMat, rowvar=False)
        db2Adj[np.where(db2Adj < np.max(dbThrsMean, 0))] = 0
        if verbose: print('db2Adj: \n',db2Adj)
        
        # Perform spectral clustering
        Lap = L(db2Adj,normalized=True, verbose=False); # Compute normalized Laplacian 
        labels, inNclusters = SC(Lap, k=None, psi=None, nrep=5, itermax=300, show_plot=True, topK = 5, verbose=False)
#         in1CluKM = SpectralClustering(n_clusters=sOPTION.inNClu, assign_labels="discretize",random_state=0,n_jobs=-1).fit_predict(db2Adj)
        print('Number of clusters: ', inNclusters[0])
        print('Clusters: ',labels)
        
        # Plot the graph
        print('Plotting the graph for the random filters method')
#         PlotGraphs(db2Adj, labels=labels)
            
        # Calculates the filters 
        sFILTER = MergeFilters(labels, bl1T_Epoch, sCLU, cFILTER, dbRate_All, in1Sel, in1CluKM, verbose)
        
        # Plot the filter for the enriched states
        print('After merging similar filters')
        PlotFilters(sFILTER.db2Filter)

        # Determines whos significant
        cSIG = statsmodels.stats.multitest.multipletests(sFILTER.dbPVal, alpha=sOPTION.dbSigThrs, method='fdr_bh')
        if verbose: print('cSIG: \n',cSIG)
        sFILTER.blSig_FDR = cSIG[0]
        
        end = time.time()
        print('Time for enrichment analysis and plots {}'.format(end - start))

    
    elif sOPTION.MergingMETHOD=='SC_with_RBF':
        '''
        Strategy 2: just Spectral clustering on the filters (using gaussian kernel for smoothing)
        '''
        print('Starting SC_with_RBF')
        start = time.time()
        
        W = gaussian_kernel(db2FiltMat.T, kernel_type="adaptive", sigma=None) # Build an adjacency matrix for data. Each column here is a filter (aka, observation)
        Lap = L(W,normalized=False); # Compute a graph laplacian
        labels, inNclusters = SC(Lap, k=None, psi=None, nrep=5, itermax=300, show_plot=True, topK = 5, verbose=False) # Compute labels and number of clusters according via spectral clustering
        print('Number of clusters: ', inNclusters[0])
        print('Clusters: ',labels)

        # Plot the graph
        print('Plotting the graph for the Spec. Clustering (with RBF) method')
#         PlotGraphs (W, labels=labels)
        
        # Calculates the filters 
        sFILTER = MergeFilters(labels, bl1T_Epoch, sCLU, cFILTER, dbRate_All, in1Sel, in1CluKM, verbose)
        
        # Plot the filter for the enriched states
        print('After merging similar filters')
        PlotFilters(sFILTER.db2Filter)
        
        end = time.time()
        print('Time for merging filters: {}'.format(end - start))
        
    
    elif sOPTION.MergingMETHOD=='louvain':
        import networkx as nx
        import community as community_louvain

        '''
        Strategy 3: Use louvain to find clusters
        '''
        start = time.time()
        print('Starting Louvain')
        W = gaussian_kernel(db2FiltMat.T, kernel_type="adaptive", sigma=None)
        G=nx.from_numpy_matrix(W)
        partition = community_louvain.best_partition(G, resolution=1,random_state=0)
        labels = np.array(list(partition.values()))
        print('Number of clusters: ', len(np.unique(labels)))
        print('Clusters: ',labels)
        
        # Plot the graph
        print('Plotting the graph with the Louvain method')
#         PlotGraphs(W, labels=labels)
        
         # Calculates the filters 
        sFILTER = MergeFilters(labels, bl1T_Epoch, sCLU, cFILTER, dbRate_All, in1Sel, in1CluKM, verbose)
        
        # Plot the filter for the enriched states
        print('After merged similar filters')
        PlotFilters(sFILTER.db2Filter)
        
        end = time.time()
        print('Time for merging filters: {}'.format(end - start))
        
    else:
        print('No method was selected')
        
    return sFILTER, sCLU, in1CluKM, in1Sel, labels
        
    



